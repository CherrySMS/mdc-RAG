{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9d3cb61f-7566-40ad-a14d-b1091fd70759",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "from dotenv import load_dotenv\n",
    "from utils.pdf_processor import PDFProcessor\n",
    "from utils.vectorizer import Vectorizer\n",
    "from utils.pinecone_manager import PineconeManager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a99e9c8-04bc-4a72-a3f8-8c3996443a94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load environment variables from .env file\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a69e67f-766f-4c70-abec-779487951218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    parser = argparse.ArgumentParser(description='PDF Course Embedding Tool')\n",
    "    # Add arguments for running the script\n",
    "    parser.add_argument('--pdf_directory', type=str, default=os.getenv('PDF_DIRECTORY'))\n",
    "    parser.add_argument('--pinecone_api_key', type=str, default=os.getenv('PINECONE_API_KEY'))\n",
    "    parser.add_argument('--pinecone_environment', type=str, default=os.getenv('PINECONE_ENVIRONMENT'))\n",
    "    parser.add_argument('--pinecone_index_name', type=str, default=os.getenv('PINECONE_INDEX_NAME'))\n",
    "    parser.add_argument('--pinecone_region', type=str, default=os.getenv('PINECONE_REGION'))\n",
    "\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    # Initialize components\n",
    "    pdf_processor = PDFProcessor(directory_path=args.pdf_directory)\n",
    "    vectorizer = Vectorizer()\n",
    "    pinecone_manager = PineconeManager(api_key=args.pinecone_api_key, \n",
    "                                       cloud=args.pinecone_environment,\n",
    "                                       region=args.pinecone_region,\n",
    "                                       index_name=args.pinecone_index_name)\n",
    "\n",
    "    # Process PDFs and prepare metadata\n",
    "    texts, metadata = pdf_processor.process_pdfs()  # Corrected method call\n",
    "\n",
    "    # Create embeddings for all texts\n",
    "    embeddings = vectorizer.create_embeddings(texts)\n",
    "    ids = [f\"doc_{i}\" for i in range(len(embeddings))]  # Assign unique IDs for each document\n",
    "\n",
    "    # Upsert the embeddings to Pinecone\n",
    "    pinecone_manager.upsert_embeddings(embeddings, ids, metadata)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8f089d2b-edad-4a22-af6e-6147532e9c66",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Retrieve Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0bc7105-a461-404f-a7b8-918e6b3e0d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import logging\n",
    "from pinecone import Pinecone\n",
    "from utils.vectorizer import Vectorizer\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# Disable parallelism in tokenizers to avoid deadlocks\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "class RAGProcessor:\n",
    "    def __init__(self):\n",
    "        load_dotenv()\n",
    "        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "        \n",
    "        self.pinecone_api_key = os.getenv('PINECONE_API_KEY')\n",
    "        self.pinecone_index_name = os.getenv('PINECONE_INDEX_NAME').lower().replace(' ', '-')\n",
    "        self.openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "        self.model_name = os.getenv('MODEL_NAME', 'gpt-4.0-turbo')\n",
    "\n",
    "        self.pc = Pinecone(api_key=self.pinecone_api_key)\n",
    "        self.index = self.pc.Index(self.pinecone_index_name)\n",
    "\n",
    "        self.vectorizer = Vectorizer()\n",
    "        self.chat_model = ChatOpenAI(api_key=self.openai_api_key, model=self.model_name)\n",
    "\n",
    "    def run_query(self, query):\n",
    "        embed = self.vectorizer.create_embeddings([query])[0]\n",
    "        vector = embed.tolist() if hasattr(embed, 'tolist') else embed\n",
    "        res = self.index.query(vector=vector, top_k=3, include_metadata=True)\n",
    "        contexts = [f\"Document {x['metadata']['filename']} might be relevant.\" for x in res['matches']]\n",
    "\n",
    "        prompt = (\"You are a helpful assistant. Based on the context provided below, \"\n",
    "                  \"generate detailed steps. \"\n",
    "                  \"Use the context to guide the user clearly and concisely:\\n\\n\" +\n",
    "                  \"\\n\\n\".join(contexts) +\n",
    "                  f\"\\n\\nQuestion: {query}\\n\\nAnswer:\")\n",
    "\n",
    "        response = self.chat_model.invoke(input=prompt, max_tokens=1500)\n",
    "\n",
    "\n",
    "        if hasattr(response, 'content'):\n",
    "            return response.content\n",
    "        return \"No detailed instructions generated.\"\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    rag_processor = RAGProcessor()\n",
    "    query = input(\"Enter your query: \")\n",
    "    response = rag_processor.run_query(query)\n",
    "    print('-' * 80)\n",
    "    print(response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
